# ğŸ§  AI Ecosystem Simulator
## Emergent Cooperation in Multi-Agent Reinforcement Learning

A real-time multi-agent reinforcement learning system where **100 autonomous AI agents** learn survival strategies and **discover cooperation organically**â€”without any hard-coded social rules.

This project demonstrates how complex social behaviors such as **cooperation, alliance formation, trust, and reciprocity** can emerge purely from **self-interested reward optimization**, mirroring patterns observed in human societies.

---

## ğŸš€ Project Overview

The AI Ecosystem Simulator places 100 agents in a shared survival environment where they must collect resources, manage health, and interact with other agents. Each agent starts with a personality-based exploration bias, but **all long-term behavior is learned**, not programmed.

Over 50 episodes, the system evolves from complete chaos into an organized society where **cooperation consistently outperforms competition**.

---

## âœ¨ Key Features

- **100 autonomous agents** in a shared 20Ã—20 grid environment  
- **Emergent cooperation** learned through reinforcement learning  
- **Policy Gradient learning** with PyTorch neural networks (~110K parameters)
- **Three personality types** influencing exploration (Cooperative, Aggressive, Neutral)  
- **Real-time visualization** with Socket.io streaming  
- **Interactive dashboard** showing live agent behavior and social metrics
- **Quantifiable social dynamics**: cooperation events, thefts, alliance formations  
- **AI-powered analysis** with Kestra workflow orchestration

---

## ğŸ­ Agent Personalities

Agents are initialized with personality-based exploration biases:

| Personality | Percentage | Exploration Bias |
|------------|-----------|------------------|
| ğŸ¤ Cooperative | 40% | Sharing resources, forming alliances, helping others |
| âš”ï¸ Aggressive | 30% | Stealing resources for short-term gain |
| âš–ï¸ Neutral | 30% | Independent survival, balanced approach |

> âš ï¸ **Important:** Personalities only influence initial exploration.  
> **No cooperation logic is explicitly programmed.** All social behavior emerges from learning.

---

## ğŸŒ Environment

- **Grid size:** 20 Ã— 20 cells
- **Agents:** 100 autonomous learners
- **Food items:** 30 (dynamically respawning)
- **Obstacles:** 50 static barriers
- **Episodes:** 50 training episodes
- **Steps per episode:** 500 steps
- **Health system:** Agents start at 100 HP, decay over time
- **Total training time:** ~15-20 minutes

---

## ğŸ® Action Space (9 Actions)

Each agent can choose from 9 discrete actions per step:

### Movement (Actions 0-4)
- **0:** Stay in place
- **1:** Move up
- **2:** Move down
- **3:** Move left
- **4:** Move right

### Social Actions (Actions 5-8)
- **5:** Share food with nearby cooperative agent
- **6:** Steal food from nearby agent
- **7:** Form/join alliance with nearby cooperative agent
- **8:** Send help signal to alliance members

---

## ğŸ‘ï¸ Observation Space (40 Dimensions)

Each agent receives a **40-dimensional observation vector**:

| Component | Dimensions | Description |
|-----------|-----------|-------------|
| Position | 2 | Agent's x, y coordinates |
| Health | 1 | Normalized health (0-1) |
| Food inventory | 1 | Amount of food held |
| Nearest food | 2 | Relative position of closest food |
| Personality | 3 | One-hot encoding (coop/agg/neutral) |
| Alliance status | 1 | Binary: in alliance or not |
| Nearby agents | 3 | Count of total, cooperative, aggressive |
| Communication | 3 | Binary signals: help, food, danger |
| Local obstacles | 9 | 3Ã—3 grid of obstacles around agent |
| Padding | 15 | Reserved for future features |

This enables **socially-aware decision making** based on nearby agents' states.

---

## ğŸ§  Learning Algorithm

- **Algorithm:** Policy Gradient with PyTorch
- **Neural Network:** 
  - Input layer: 40 dimensions
  - Hidden layers: 256 â†’ 256 â†’ 128 neurons
  - Output layer: 9 actions (discrete)
  - **Total parameters:** ~110,345
- **Optimizer:** Adam (learning rate: 3e-4)
- **Exploration:** Îµ-greedy (starts at 1.0, decays to 0.1)
- **Update frequency:** Every 20 steps with mini-batch learning

---

## ğŸ¯ Reward Design

Carefully crafted rewards encourage emergent cooperation:

| Action | Immediate Reward | Long-Term Effect |
|--------|-----------------|------------------|
| Collect food | +15 | Health restoration (+10 HP) |
| Step penalty | -1 | Encourages efficient behavior |
| Hit obstacle | -5 | Health damage (-2 HP) |
| **Share food** | +5 (both agents) | Builds trust, reciprocity |
| **Steal food** | +10 (thief), -10 (victim) | Short-term gain, long-term isolation |
| **Form alliance** | +3 (both agents) | Enables coordination |
| **Alliance bonus** | +0.1 HP/step | Sustained survival advantage |
| Health decay | -0.2 HP/step | Creates urgency for cooperation |

**Key Insight:** The reward structure makes cooperation mathematically optimal over time, but agents must discover this through exploration.

---

## ğŸ“ˆ Emergent Results

### Quantitative Metrics

| Metric | Episode 1 | Episode 50 | Change |
|--------|-----------|-----------|--------|
| Cooperation events | ~15 | ~80 | +433% |
| Theft events | ~35 | ~10 | -71% |
| Survival rate | 23% | 68% | +196% |
| Stable alliances | 0 | 10-15 | âˆ |
| Average health | 45 | 78 | +73% |
| Food collected | 500 | 1800 | +260% |

### Behavioral Observations

**Early Episodes (1-10):**
- Random exploration dominates
- High theft rate (aggressive agents exploit)
- Low survival rates
- No stable alliances
- Agents die quickly from health decay

**Mid Episodes (11-30):**
- Cooperative agents begin clustering
- Alliance formations increase
- Theft becomes less profitable (victims avoid thieves)
- Survival rates improve

**Late Episodes (31-50):**
- Stable cooperative clusters form
- Defensive alliances patrol shared territory
- Aggressive agents isolated at periphery
- Neutral agents find ecological niches
- **Cooperation becomes the dominant strategy**

---

## ğŸ–¥ï¸ Real-Time Visualization

### Technology Stack
- **Frontend:** React with Canvas rendering
- **Backend:** Node.js + Express
- **Streaming:** Socket.io (WebSocket)
- **Charts:** Recharts for analytics
- **Frame rate:** Updates every 10 steps (~30 FPS effective)

### Visual Features
- **Color-coded agents** by personality:
  - ğŸŸ¢ Green = Cooperative
  - ğŸ”´ Red = Aggressive
  - ğŸ”µ Blue = Neutral
- **Health bars** above each agent
- **Alliance rings** (colored halos around allied agents)
- **Communication signals** (dots above agents)
- **Real-time graphs:**
  - Social interactions over time
  - Personality performance comparison
  - Current episode progress

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Browser                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          React Dashboard (Port 3000)             â”‚  â”‚
â”‚  â”‚  â€¢ Canvas visualization                          â”‚  â”‚
â”‚  â”‚  â€¢ Real-time charts                              â”‚  â”‚
â”‚  â”‚  â€¢ Socket.io client                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ WebSocket
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Node.js Server (Port 4000)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Socket.io Bridge                       â”‚  â”‚
â”‚  â”‚  â€¢ Receives simulation updates                   â”‚  â”‚
â”‚  â”‚  â€¢ Broadcasts to React clients                   â”‚  â”‚
â”‚  â”‚  â€¢ Stores episode history                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Socket.io
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Python Simulation                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Gymnasium environment                         â”‚  â”‚
â”‚  â”‚  â€¢ PyTorch neural networks                       â”‚  â”‚
â”‚  â”‚  â€¢ 100 agent policy gradient learning            â”‚  â”‚
â”‚  â”‚  â€¢ Socket.io client                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Kestra (Optional)                              â”‚
â”‚  â€¢ AI-powered analysis with Claude                      â”‚
â”‚  â€¢ Automated report generation                          â”‚
â”‚  â€¢ Scheduled workflow execution                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure
```
ai-ecosystem-simulator/
â”œâ”€â”€ client/                    # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js            # Main dashboard component
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ server/                    # Node.js backend
â”‚   â”œâ”€â”€ index.js              # Socket.io server
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ simulation/                # Python RL training
â”‚   â”œâ”€â”€ main.py               # Training loop
â”‚   â”œâ”€â”€ env_logic_social.py   # Gymnasium environment
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ kestra/                    # Workflow orchestration
â”‚   â””â”€â”€ summary_flow.yaml     # AI analysis workflow
â”œâ”€â”€ data/                      # Training data
â”‚   â””â”€â”€ survival_data.json
â””â”€â”€ README.md
```

---

## ğŸ§ª Research Challenges Addressed

1. **Non-stationary environments:** Each agent's learning affects others' optimal policies
2. **Credit assignment problem:** Which agent's actions led to cooperative success?
3. **Scalability:** Training 100 agents simultaneously (900-dimensional joint action space)
4. **Exploration vs exploitation:** Balancing personality-biased exploration with learned behavior
5. **Emergent behavior:** No explicit cooperation rulesâ€”all social behavior must emerge

---

## ğŸŒ Real-World Applications

- **Multi-robot coordination:** Warehouse robots, drone swarms
- **Autonomous vehicles:** Traffic flow optimization, cooperative driving
- **Economic simulations:** Market dynamics, resource allocation
- **Ecosystem modeling:** Predator-prey dynamics, species cooperation
- **Game AI:** Adaptive NPCs with realistic social behavior
- **AI safety research:** Understanding emergence of cooperation vs competition

---

## ğŸš€ Getting Started

### Prerequisites
- **Node.js** (v18+)
- **Python** (3.9+)
- **PyTorch** (2.0+)

### Installation

```bash
# Clone repository
git clone https://github.com/your-username/ai-ecosystem-simulator
cd ai-ecosystem-simulator

# Install backend dependencies
cd server
npm install

# Install frontend dependencies
cd ../client
npm install

# Install Python dependencies
cd ../simulation
pip install -r requirements.txt
```

### Running Locally

```bash
# Terminal 1: Start backend
cd server
npm start

# Terminal 2: Start simulation
cd simulation
python main.py

# Terminal 3: Start frontend
cd client
npm start
```

Open http://localhost:3000 to view the dashboard.

---

## ğŸ“Š Key Metrics to Watch

- **Cooperation Events:** Number of food-sharing actions
- **Theft Events:** Number of stealing actions
- **Alliances Formed:** Stable partnerships between agents
- **Survival Rate:** Percentage of agents with score > 0
- **Average Health:** Overall agent well-being
- **Personality Performance:** Which strategy works best?

---

## ğŸ§  Key Insight

> **Cooperation was never programmed.  
> Cooperation was learned.**

This project demonstrates that cooperation is not merely ethicalâ€”it is **mathematically optimal** under appropriate incentive structures. Given the right reward environment, even purely self-interested agents discover that cooperation yields higher long-term rewards than competition.

---

## ğŸŒ Live Demo & Repository

- **Live Demo:** [Coming Soon - Deploy on Railway/Vercel]
- **GitHub Repository:** https://github.com/your-username/ai-ecosystem-simulator

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | React, Socket.io-client, Recharts, Canvas API |
| Backend | Node.js, Express, Socket.io |
| ML/AI | Python, PyTorch, Gymnasium, NumPy |
| Orchestration | Kestra, Anthropic Claude API |
| Deployment | Vercel (Frontend), Railway (Backend + Simulation) |
| Version Control | Git, GitHub |

---

## ğŸ“š Future Enhancements

- [ ] Add more personality types (Defensive, Opportunistic, etc.)
- [ ] Implement communication language learning
- [ ] Add evolutionary component (successful agents reproduce)
- [ ] 3D visualization with Three.js
- [ ] Multi-species ecosystems
- [ ] Reputation system
- [ ] Trading economy between agents
- [ ] GPU acceleration for larger populations (1000+ agents)

---

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the MIT License 

---

## ğŸ™ Acknowledgements

- **PyTorch** team for the deep learning framework
- **Gymnasium** (formerly OpenAI Gym) for RL environments
- **Anthropic** for Claude API used in Kestra analysis
- **Socket.io** for real-time communication
- Multi-agent RL research community
- Open-source contributors
